{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:19:54.753456: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-10 16:19:54.767409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731248394.785252   13244 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731248394.791268   13244 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-10 16:19:54.808802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-string entries found in 'text' column:\n",
      "      text  label\n",
      "294    NaN      9\n",
      "11581  NaN      8\n",
      "Data is ready for tokenization: 'text' and 'label' columns are consistent in length.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = '/home/moegho/Desktop/490_Project/Data_processing/clean_labeled_news_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Check for non-string entries in the 'text' column and remove them\n",
    "non_string_entries = df[~df['text'].apply(lambda x: isinstance(x, str))]\n",
    "if not non_string_entries.empty:\n",
    "    print(\"Non-string entries found in 'text' column:\")\n",
    "    print(non_string_entries)\n",
    "    # Remove non-string entries from 'text' column\n",
    "    df = df[df['text'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)\n",
    "\n",
    "# Step 2: Reindex the 'label' column to match the filtered 'text' column\n",
    "# This step ensures that the label column only includes rows with valid text entries\n",
    "df['label'] = df['label'].iloc[df.index].reset_index(drop=True)\n",
    "\n",
    "# Final consistency check between 'text' and 'label' columns\n",
    "if len(df['text']) != len(df['label']):\n",
    "    raise ValueError(\"Inconsistent lengths after filtering. Please check the data.\")\n",
    "else:\n",
    "    print(\"Data is ready for tokenization: 'text' and 'label' columns are consistent in length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the 'text' column\n",
    "encodings = tokenizer(\n",
    "    df['text'].tolist(), \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=128, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Prepare the labels as tensors\n",
    "labels = torch.tensor(df['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized data and labels\n",
    "torch.save(encodings['input_ids'], 'input_ids.pt')\n",
    "torch.save(encodings['attention_mask'], 'attention_mask.pt')\n",
    "torch.save(labels, 'labels.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.load('input_ids.pt')\n",
    "# attention_mask = torch.load('attention_mask.pt')\n",
    "# labels = torch.load('labels.pt')\n",
    "\n",
    "# for loading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
